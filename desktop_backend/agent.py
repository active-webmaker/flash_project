import os
import re
import uuid
import time
import logging
from logging.handlers import RotatingFileHandler
import json
from datetime import datetime
from collections import defaultdict
import operator
from typing import Annotated, Sequence, TypedDict

import requests
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage
from langchain_core.tools import StructuredTool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor

from git_analyzer import GitAnalyzer
from git_commit_module import GitCommitModule

# ë¡œê±° ì„¤ì • (íŒŒì¼ + ì½˜ì†”)
LOG_DIR = "/app/log"
os.makedirs(LOG_DIR, exist_ok=True)

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(formatter)
root_logger.addHandler(console_handler)

# íŒŒì¼ í•¸ë“¤ëŸ¬
log_file = os.path.join(LOG_DIR, "agent.log")
file_handler = RotatingFileHandler(
    log_file,
    maxBytes=10 * 1024 * 1024,  # 10MB
    backupCount=10
)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)
root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)

load_dotenv()

API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8000/api/v1")
LOCAL_LLM_URL = os.getenv("LOCAL_LLM_URL", "http://127.0.0.1:8001/v1")
REPO_PATH = os.getenv("REPO_PATH", "./test_repo")

AGENT_VERSION = os.getenv("AGENT_VERSION", "v1.0.0")
AGENT_ID = os.getenv("AGENT_ID", f"agent-py-{uuid.uuid4()}")

job_metrics = defaultdict(dict)


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    job_id: str
    job_description: str
    job_payload: dict


def utc_now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def safe_json_loads(value):
    if isinstance(value, str):
        try:
            return json.loads(value)
        except json.JSONDecodeError:
            return value
    return value


def ensure_jsonable(value):
    if isinstance(value, (dict, list, int, float, bool)) or value is None:
        return value
    return str(value)


def create_structured_tools(git_analyzer, git_commit_module):
    """
    ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œë¥¼ StructuredToolë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    tools = [
        StructuredTool.from_function(
            func=git_analyzer.scan_file_tree,
            name="scan_file_tree",
            description="ë¡œì»¬ ì €ì¥ì†Œì˜ íŒŒì¼/ë””ë ‰í„°ë¦¬ íŠ¸ë¦¬ë¥¼ JSON-í˜¸í™˜ dictë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
        ),
        StructuredTool.from_function(
            func=git_analyzer.calculate_loc_per_language,
            name="calculate_loc_per_language",
            description="ì €ì¥ì†Œ ë‚´ ê° í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ì½”ë“œ ë¼ì¸ ìˆ˜(LOC)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."
        ),
        StructuredTool.from_function(
            func=git_commit_module.create_commit,
            name="create_commit",
            description="ì§€ì •ëœ íŒŒì¼ë“¤ì„ ìŠ¤í…Œì´ì§•í•˜ê³  ìƒˆ ì»¤ë°‹ì„ ìƒì„±í•©ë‹ˆë‹¤."
        ),
        StructuredTool.from_function(
            func=git_commit_module.get_diff,
            name="get_diff",
            description="íŠ¹ì • ì»¤ë°‹ ë˜ëŠ” HEADì˜ ë³€ê²½ ì‚¬í•­(diff)ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
        ),
    ]
    return tools


def build_job_prompt(job_payload: dict, job_type: str) -> str:
    """
    LLMì´ tool-callingì„ ë” ì˜ ìˆ˜í–‰í•˜ë„ë¡ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì§€ì •ëœ ë„êµ¬ëª…ì´ ìˆëŠ” ê²½ìš° ìš°ì„  ì‚¬ìš©
    tool_name = job_payload.get('tool_name')

    if tool_name:
        # ë„êµ¬ê°€ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ ê²½ìš°
        prompt_template = f"""ë‹¹ì‹ ì€ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

## ì§€ì‹œì‚¬í•­
1. ì•„ë˜ì˜ **ì§€ì •ëœ ë„êµ¬**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
2. ë‹¤ë¥¸ ë„êµ¬ê°€ ì•„ë‹Œ, ì •í™•íˆ ëª…ì‹œëœ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ì„¸ìš”.
3. ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì—†ì´, ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” JSON ê°ì²´ë§Œì„ ì‘ë‹µìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

## ì§€ì •ëœ ë„êµ¬
{tool_name}

## ì‘ë‹µ í˜•ì‹
{{"name": "{tool_name}", "arguments": {{}}}}
"""
    else:
        # ë„êµ¬ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° (ì¼ë°˜ì ì¸ repository_analysis)
        description = (
            job_payload.get('description')
            or job_payload.get('prompt')
            or job_payload.get('title')
            or job_type
        )

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
        tool_definitions = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])

        # ìµœì¢… í”„ë¡¬í”„í…œí”Œë¦¿
        prompt_template = f"""ë‹¹ì‹ ì€ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

## ì§€ì‹œì‚¬í•­
1. ì•„ë˜ `ì‘ì—… ë‚´ìš©`ì„ ë¶„ì„í•˜ì„¸ìš”.
2. `ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬` ëª©ë¡ì—ì„œ ì‘ì—…ì„ í•´ê²°í•˜ëŠ” ë° ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ **í•˜ë‚˜ë§Œ** ì„ íƒí•˜ì„¸ìš”.
3. ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì—†ì´, ì„ íƒí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” JSON ê°ì²´ë§Œì„ ì‘ë‹µìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

## ì‘ì—… ë‚´ìš©
{description}

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
{tool_definitions}

## ì‘ë‹µ í˜•ì‹
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

- ì¸ìˆ˜ê°€ ì—†ëŠ” ë„êµ¬ì˜ ê²½ìš°:
{{"name": "ë„êµ¬_ì´ë¦„", "arguments": {{}}}}

- ì¸ìˆ˜ê°€ ìˆëŠ” ë„êµ¬ì˜ ê²½ìš°:
{{"name": "ë„êµ¬_ì´ë¦„", "arguments": {{"ì¸ìˆ˜_ì´ë¦„": "ê°’", ...}}}}
"""
    
    # payloadì˜ ë‹¤ë¥¸ ì •ë³´ë“¤ì„ ì¶”ê°€ (ì„ íƒì )
    # project_section = job_payload.get('project')
    # if isinstance(project_section, dict) and project_section:
    #     prompt_template += "\n\n## í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸\n" + json.dumps(project_section, indent=2, ensure_ascii=False)

    return prompt_template


def init_test_repo_with_samples(repo_path: str):
    """í…ŒìŠ¤íŠ¸ìš© repoë¥¼ ìƒ˜í”Œ íŒŒì¼ê³¼ í•¨ê»˜ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    from git import Repo

    os.makedirs(repo_path, exist_ok=True)

    # ì´ë¯¸ git repoì¸ì§€ í™•ì¸
    git_dir = os.path.join(repo_path, '.git')
    if not os.path.exists(git_dir):
        Repo.init(repo_path)
        logger.info(f"Git repository initialized at {repo_path}")

    # ìƒ˜í”Œ íŒŒì¼ êµ¬ì¡° ìƒì„±
    sample_files = {
        "README.md": "# Test Repository\n\nThis is a test repository for git analysis.\n",
        "src/main.py": "def hello():\n    print('Hello, World!')\n\nif __name__ == '__main__':\n    hello()\n",
        "src/utils.py": "def add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n",
        "tests/test_main.py": "import unittest\nfrom src.main import hello\n\nclass TestMain(unittest.TestCase):\n    def test_hello(self):\n        self.assertTrue(True)\n",
        "config.json": '{"name": "test-project", "version": "1.0.0"}\n',
        ".gitignore": "__pycache__/\n*.pyc\n.venv/\n",
    }

    # íŒŒì¼ ìƒì„±
    files_created = False
    for file_path, content in sample_files.items():
        full_path = os.path.join(repo_path, file_path)
        dir_path = os.path.dirname(full_path)

        # ë””ë ‰í„°ë¦¬ ìƒì„±
        os.makedirs(dir_path, exist_ok=True)

        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(full_path):
            with open(full_path, "w") as f:
                f.write(content)
            logger.debug(f"Created sample file: {file_path}")
            files_created = True

    if files_created:
        logger.info(f"Sample files created in {repo_path}")

        # ìƒ˜í”Œ íŒŒì¼ì„ gitì— ì»¤ë°‹
        try:
            repo = Repo(repo_path)

            # Git ì‚¬ìš©ì ì„¤ì • (Docker í™˜ê²½ì—ì„œëŠ” í•„ìš”)
            try:
                with repo.config_reader() as git_config:
                    git_config.get_value("user", "name")
            except:
                # ì‚¬ìš©ì ì„¤ì •ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                repo.config_writer().set_value("user", "name", "Agent Bot").release()
                repo.config_writer().set_value("user", "email", "agent@bot.local").release()
                logger.debug("Git user config set")

            # ìƒì„±ëœ ëª¨ë“  íŒŒì¼ì„ ìŠ¤í…Œì´ì§• (ëª…ì‹œì  ë¦¬ìŠ¤íŠ¸)
            files_to_add = list(sample_files.keys())
            repo.index.add(files_to_add)
            logger.debug(f"Staged files: {files_to_add}")

            # ì»¤ë°‹ ìƒì„±
            repo.index.commit("Initial commit with sample files")
            logger.info(f"Sample files committed to git")
        except Exception as e:
            logger.warning(f"Failed to commit sample files: {e}", exc_info=True)
    else:
        # íŒŒì¼ì´ ì´ë¯¸ ìˆì–´ë„ ì»¤ë°‹ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
        try:
            repo = Repo(repo_path)

            # Git ì‚¬ìš©ì ì„¤ì • í™•ì¸
            try:
                with repo.config_reader() as git_config:
                    git_config.get_value("user", "name")
            except:
                repo.config_writer().set_value("user", "name", "Agent Bot").release()
                repo.config_writer().set_value("user", "email", "agent@bot.local").release()

            # ì»¤ë°‹í•  íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if repo.index.diff(None) or repo.untracked_files:
                # ëª¨ë“  íŒŒì¼ì„ ì¶”ê°€
                untracked = repo.untracked_files
                repo.index.add(untracked)
                logger.debug(f"Staged untracked files: {untracked}")

                repo.index.commit("Commit pending sample files")
                logger.info(f"Pending sample files committed to git")
        except Exception as e:
            logger.debug(f"No pending files to commit: {e}")


# repo ì´ˆê¸°í™” ë° ìƒ˜í”Œ íŒŒì¼ ìƒì„±
if not os.path.exists(REPO_PATH):
    logger.warning(f"Test repository not found at {REPO_PATH}. Initializing a new one.")
    init_test_repo_with_samples(REPO_PATH)
else:
    # ê¸°ì¡´ repoê°€ ìˆì–´ë„ ìƒ˜í”Œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(os.path.join(REPO_PATH, "src")):
        logger.info(f"Adding sample files to existing repository at {REPO_PATH}")
        init_test_repo_with_samples(REPO_PATH)


git_analyzer = GitAnalyzer(repo_path=REPO_PATH)
git_commit_module = GitCommitModule(repo_path=REPO_PATH)


# ì „ì—­ toolsë¥¼ StructuredToolë¡œ ë³€í™˜
tools = create_structured_tools(git_analyzer, git_commit_module)
tool_executor = ToolExecutor(tools)


llm = ChatOpenAI(
    openai_api_base=LOCAL_LLM_URL,
    openai_api_key="dummy_key",
    temperature=0,
    streaming=True,
)

# ë„êµ¬ ì„ íƒìš© LLM (tool-calling í™œì„±í™”)
llm_with_tools = llm.bind_tools(tools)

# ë¶„ì„ìš© LLM (ìˆœìˆ˜ ì±„íŒ…, tool-calling ì—†ìŒ)
llm_for_analysis = ChatOpenAI(
    openai_api_base=LOCAL_LLM_URL,
    openai_api_key="dummy_key",
    temperature=0,
    streaming=False,
)


def should_continue(state: AgentState):
    messages = state['messages']
    last_message = messages[-1]
    # If the LLM makes a tool call, then we route to the "action" node
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"
    # Otherwise, we end the conversation
    return "end"


def call_model(state: AgentState):
    """
    LLMì„ í˜¸ì¶œí•˜ê³ , ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ë„êµ¬ í˜¸ì¶œì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ AIMessageë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    messages = state['messages']
    response = llm_with_tools.invoke(messages)
    
    # ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ JSON ë¸”ë¡ì„ ì°¾ìœ¼ë ¤ëŠ” ì‹œë„
    try:
        # ì‘ë‹µ ë‚´ìš©ì—ì„œ ```json ... ``` ë¸”ë¡ ë˜ëŠ” ì¼ë°˜ JSON ê°ì²´ ì¶”ì¶œ
        content = response.content
        match = re.search(r"```json\s*([\s\S]*?)\s*```|({[\s\S]*}*)", content)

        if match:
            json_str = match.group(1) or match.group(2)
            tool_call_data = json.loads(json_str)

            # LangChainì˜ ToolCall í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if "name" in tool_call_data and "arguments" in tool_call_data:
                response.tool_calls = [
                    {
                        "id": f"tool_call_{uuid.uuid4()}",
                        "name": tool_call_data["name"],
                        "args": tool_call_data["arguments"],
                    }
                ]
                logger.info(f"âœ… ëª¨ë¸ ì‘ë‹µì—ì„œ Tool Callì„ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤: {tool_call_data['name']}")
    except (json.JSONDecodeError, AttributeError) as e:
        logger.warning(f"âš ï¸ ëª¨ë¸ ì‘ë‹µì—ì„œ Tool Callì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, tool_callsê°€ ì—†ëŠ” ì›ë˜ ì‘ë‹µì„ ë°˜í™˜
        pass

    return {"messages": [response]}


from langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation

# ... (rest of the imports)

# ... (code before call_tool)

def call_tool(state: AgentState, executor=None):
    """Global tool executorë¥¼ ì‚¬ìš©í•˜ëŠ” ë²„ì „"""
    messages = state['messages']
    last_message = messages[-1]

    # tool_callsëŠ” LangChainì´ ìƒì„±í•˜ëŠ” í‘œì¤€ ì†ì„±ì…ë‹ˆë‹¤.
    tool_invocations = []
    if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
        logger.warning("call_tool ë…¸ë“œì— ë„ë‹¬í–ˆì§€ë§Œ, ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— tool_callsê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"messages": [HumanMessage(content="ëª¨ë¸ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì‘ë‹µì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")]}

    for tool_call in last_message.tool_calls:
        tool_name = tool_call.get("name")
        parsed_args = tool_call.get("args")
        
        logger.info(f"ë„êµ¬ í˜¸ì¶œ: {tool_name} (ì¸ìˆ˜: {parsed_args})")
        report_job_progress(state['job_id'], log_message=f"Calling tool '{tool_name}'")
        report_tool_callback(state['job_id'], tool_name, parsed_args)
        
        # ToolExecutorê°€ ToolInvocation ê°ì²´ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
        tool_invocations.append(
            ToolInvocation(tool=tool_name, tool_input=parsed_args)
        )

    # ëª¨ë“  ë„êµ¬ë¥¼ ì‹¤í–‰
    responses = tool_executor.batch(tool_invocations)
    
    # ê° ì‹¤í–‰ ê²°ê³¼ë¥¼ ToolMessageë¡œ ë³€í™˜
    tool_messages = []
    for tool_call, response in zip(last_message.tool_calls, responses):
        tool_messages.append(
            ToolMessage(content=str(response), tool_call_id=tool_call.get("id"))
        )
        report_tool_callback(state['job_id'], tool_call.get("name"), tool_call.get("args"), tool_output=ensure_jsonable(response))

    job_metrics.setdefault(state['job_id'], {"tool_calls": 0, "started_at": time.time()})
    job_metrics[state['job_id']]['tool_calls'] += len(tool_invocations)

    return {"messages": tool_messages}


def call_tool_with_executor(state: AgentState, executor):
    """Job-specific tool executorë¥¼ ì‚¬ìš©í•˜ëŠ” ë²„ì „"""
    messages = state['messages']
    last_message = messages[-1]

    if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
        logger.warning("call_tool_with_executor ë…¸ë“œì— ë„ë‹¬í–ˆì§€ë§Œ, ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— tool_callsê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"messages": [HumanMessage(content="ëª¨ë¸ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì‘ë‹µì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")]}
        
    tool_invocations = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call.get("name")
        parsed_args = tool_call.get("args")

        logger.info(f"Job-specific ë„êµ¬ í˜¸ì¶œ: {tool_name} (ì¸ìˆ˜: {parsed_args})")
        report_job_progress(state['job_id'], log_message=f"Calling tool '{tool_name}'")
        report_tool_callback(state['job_id'], tool_name, parsed_args)

        tool_invocations.append(
            ToolInvocation(tool=tool_name, tool_input=parsed_args)
        )

    responses = executor.batch(tool_invocations)
    
    tool_messages = []
    for tool_call, response in zip(last_message.tool_calls, responses):
        tool_messages.append(
            ToolMessage(content=str(response), tool_call_id=tool_call.get("id"))
        )
        report_tool_callback(state['job_id'], tool_call.get("name"), tool_call.get("args"), tool_output=ensure_jsonable(response))

    job_metrics.setdefault(state['job_id'], {"tool_calls": 0, "started_at": time.time()})
    job_metrics[state['job_id']]['tool_calls'] += len(tool_invocations)

    return {"messages": tool_messages}


def analyze_tool_results(state: AgentState, llm):
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ LLMì´ ë¶„ì„í•˜ê³  í•´ì„í•©ë‹ˆë‹¤."""
    try:
        messages = state['messages']

        if not messages:
            logger.warning("ë©”ì‹œì§€ê°€ ì—†ì–´ì„œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"messages": []}

        # ë§ˆì§€ë§‰ ToolMessage ì°¾ê¸°
        tool_message = None
        for msg in reversed(messages):
            if isinstance(msg, ToolMessage):
                tool_message = msg
                break

        if not tool_message:
            logger.warning("ë¶„ì„í•  ToolMessageë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"messages": []}

        # ë„êµ¬ëª… ì°¾ê¸°
        tool_name = None
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    if tool_call.get('id') == tool_message.tool_call_id:
                        tool_name = tool_call.get('name')
                        break
                if tool_name:
                    break

        # ë„êµ¬ë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        analysis_prompts = {
            'calculate_loc_per_language': """ë‹¤ìŒì€ ì €ì¥ì†Œì˜ ì–¸ì–´ë³„ ì½”ë“œ ë¼ì¸ ìˆ˜(LOC) ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.

ê²°ê³¼: {result}

ì´ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ë¶„ì„í•˜ê³  í•´ì„í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
- ì–´ë–¤ ì–¸ì–´ê°€ ê°€ì¥ ë§ì€ê°€?
- í”„ë¡œì íŠ¸ì˜ ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€?
- ê° ì–¸ì–´ì˜ ë¹„ìœ¨ì€ ì–´ëŠ ì •ë„ì¸ê°€?
""",
            'get_diff': """ë‹¤ìŒì€ ì €ì¥ì†Œì˜ ë³€ê²½ ì‚¬í•­(Diff) ì¡°íšŒ ê²°ê³¼ì…ë‹ˆë‹¤.

ê²°ê³¼: {result}

ì´ Diffë¥¼ ìì—°ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
- ì–´ë–¤ íŒŒì¼ë“¤ì´ ë³€ê²½ë˜ì—ˆëŠ”ê°€?
- ì£¼ìš” ë³€ê²½ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€?
- ë³€ê²½ ê·œëª¨ëŠ” ì–´ëŠ ì •ë„ì¸ê°€?
"""
        }

        # ë„êµ¬ë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if tool_name in analysis_prompts:
            analysis_prompt = analysis_prompts[tool_name].format(result=tool_message.content)
            logger.info(f"ë„êµ¬ ê²°ê³¼ ë¶„ì„ ì¤‘: {tool_name}")

            # LLMìœ¼ë¡œ ë¶„ì„
            analysis_response = llm.invoke([
                HumanMessage(content=analysis_prompt)
            ])

            logger.info(f"ë¶„ì„ ì™„ë£Œ: {tool_name}")
            return {"messages": [analysis_response]}
        else:
            logger.debug(f"ë¶„ì„ ëŒ€ìƒì´ ì•„ë‹Œ ë„êµ¬: {tool_name}, ë¶„ì„ ìŠ¤í‚µ")
            return {"messages": []}

    except Exception as e:
        logger.error(f"ë„êµ¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return {"messages": []}

# ... (rest of the file)



def report_job_status(job_id, phase, summary=None, result_url=None, error_message=None, job_status=None):
    endpoint = f"{API_BASE_URL}/agent/jobs/{job_id}/{phase}"
    payload = {"agent_id": AGENT_ID}

    if phase == 'start':
        payload['start_time'] = utc_now_iso()
    elif phase == 'complete':
        status_value = job_status or ('failed' if error_message else 'success')
        payload['status'] = status_value
        if summary is not None:
            payload['summary'] = summary
        if result_url is not None:
            payload['final_result_url'] = result_url
        if error_message is not None:
            payload['error_message'] = error_message
    else:
        logger.debug(f"Unsupported job phase '{phase}'")
        return

    try:
        requests.post(endpoint, json=payload).raise_for_status()
        logger.info(f"Reported job {job_id} phase '{phase}'")
    except requests.RequestException as exc:
        logger.error(f"Failed to report job phase '{phase}' for job {job_id}: {exc}")


def report_job_progress(job_id, log_message=None, percent_complete=None, intermediate_artifact=None):
    endpoint = f"{API_BASE_URL}/agent/jobs/{job_id}/progress"
    payload = {"agent_id": AGENT_ID}
    if log_message is not None:
        payload['log_message'] = log_message
    if percent_complete is not None:
        payload['percent_complete'] = percent_complete
    if intermediate_artifact is not None:
        payload['intermediate_artifact'] = intermediate_artifact

    try:
        requests.post(endpoint, json=payload).raise_for_status()
    except requests.RequestException as exc:
        logger.debug(f"Failed to report progress for job {job_id}: {exc}")


def report_tool_callback(job_id, tool_name, tool_input, tool_output=None):
    endpoint = f"{API_BASE_URL}/agent/callbacks/tool"
    payload = {
        'run_id': str(job_id),
        'tool_name': tool_name,
        'tool_input': ensure_jsonable(tool_input),
    }
    if tool_output is not None:
        payload['tool_output'] = ensure_jsonable(tool_output)

    try:
        requests.post(endpoint, json=payload).raise_for_status()
    except requests.RequestException as exc:
        logger.debug(f"Failed to report tool callback for job {job_id}: {exc}")


def report_telemetry(job_id, metrics):
    endpoint = f"{API_BASE_URL}/agent/telemetry"
    metrics_list = []
    if 'tool_calls' in metrics:
        metrics_list.append({
            'name': 'tool_calls',
            'value': float(metrics.get('tool_calls', 0)),
            'job_id': str(job_id),
        })
    if 'duration_ms' in metrics:
        metrics_list.append({
            'name': 'job_duration_ms',
            'value': float(metrics.get('duration_ms', 0)),
            'job_id': str(job_id),
        })

    if not metrics_list:
        metrics_list.append({'name': 'job_duration_ms', 'value': 0.0, 'job_id': str(job_id)})

    payload = {
        'agent_id': AGENT_ID,
        'metrics': metrics_list,
    }

    try:
        requests.post(endpoint, json=payload).raise_for_status()
    except requests.RequestException as exc:
        logger.debug(f"Failed to report telemetry for job {job_id}: {exc}")


def send_heartbeat(status_value, current_job_id=None):
    endpoint = f"{API_BASE_URL}/agent/heartbeat"
    payload = {
        'agent_id': AGENT_ID,
        'status': status_value,
        'agent_version': AGENT_VERSION,
    }
    if current_job_id:
        payload['current_job_id'] = str(current_job_id)

    try:
        requests.post(endpoint, json=payload).raise_for_status()
    except requests.RequestException as exc:
        logger.debug(f"Heartbeat failed: {exc}")


workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("action", call_tool)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", "agent")
app = workflow.compile()


def run_agent():
    logger.info("=" * 80)
    logger.info(f"ğŸš€ Starting agent {AGENT_ID} (version {AGENT_VERSION})...")
    logger.info("=" * 80)
    logger.info(f"API Server: {API_BASE_URL}")
    logger.info(f"Local LLM: {LOCAL_LLM_URL}")
    logger.info(f"Repository: {REPO_PATH}")
    logger.info("=" * 80)

    while True:
        try:
            send_heartbeat('idle')
            request_payload = {
                'agent_id': AGENT_ID,
                'capabilities': [tool.name for tool in tools],
                'status': 'idle',
                'max_jobs': 1,
                'agent_version': AGENT_VERSION,
            }
            logger.debug("ğŸ“¤ Job ìš”ì²­ ì¤‘...")
            response = requests.post(f"{API_BASE_URL}/agent/jobs/request", json=request_payload)

            if response.status_code == 204:
                logger.debug("â³ Job ì—†ìŒ. ëŒ€ê¸° ì¤‘...")
                time.sleep(10)
                continue
            if response.status_code != 200:
                logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ {response.status_code}: {response.text}")
                time.sleep(10)
                continue

            try:
                payload = response.json()
            except ValueError:
                logger.warning("âš ï¸ Job ìš”ì²­ ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹˜")
                time.sleep(10)
                continue

            jobs = payload.get('jobs') if isinstance(payload, dict) else None
            if not jobs:
                logger.debug("â³ Job ì—†ìŒ. ëŒ€ê¸° ì¤‘...")
                time.sleep(10)
                continue

            job = jobs[0]
            job_id = job.get('job_id') or job.get('id')
            logger.debug(f" ìˆ˜ì‹ ëœ JOB í˜ì´ë¡œë“œ: {job}")
            if job_id is None:
                logger.warning("âš ï¸ Job ID ì—†ìŒ. ìŠ¤í‚µ...")
                time.sleep(10)
                continue

            job_payload = job.get('payload', {}) or {}
            job_type = job.get('job_type', '')
            
            logger.info("=" * 80)
            logger.info(f"âœ… ìƒˆ Job ìˆ˜ì‹ : {job_id}, íƒ€ì…: {job_type}")
            logger.info("=" * 80)

            # --- ê²½ë¡œ ë³€í™˜ ë¡œì§ (ëª¨ë“  Job ìœ í˜•ì— ê³µí†µ) ---
            project_local_path = os.path.normpath(REPO_PATH)
            logger.info(f"Job path overridden to use REPO_PATH: '{project_local_path}'")
            
            # ì´ jobì˜ GitAnalyzerì™€ GitCommitModuleì„ ìƒˆë¡œ ìƒì„±
            job_git_analyzer = GitAnalyzer(repo_path=project_local_path)
            job_git_commit_module = GitCommitModule(repo_path=project_local_path)

            # StructuredToolë¡œ ë³€í™˜í•˜ì—¬ self ë°”ì¸ë”© ë¬¸ì œ í•´ê²°
            job_tools = create_structured_tools(job_git_analyzer, job_git_commit_module)
            job_tool_executor = ToolExecutor(job_tools)
            # --- ê²½ë¡œ ë³€í™˜ ë¡œì§ ë ---

            # Job ìœ í˜•ì— ë”°ë¼ ë¶„ê¸°
            # repository_analysisì—ì„œ tool_nameì´ ëª…ì‹œëœ ê²½ìš°ë„ direct_tool_callì²˜ëŸ¼ ì²˜ë¦¬
            is_direct_tool_call = (job_type == 'direct_tool_call') or (job_type == 'repository_analysis' and job_payload.get('tool_name'))

            if is_direct_tool_call:
                logger.info(f"ğŸš€ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ Job ì²˜ë¦¬ ì‹œì‘: {job_id}")
                send_heartbeat('processing', current_job_id=job_id)
                report_job_status(job_id, 'start')

                try:
                    tool_name = job_payload.get("tool_name")
                    tool_args = job_payload.get("tool_args", {})
                    
                    if not tool_name:
                        raise ValueError("Payloadì— 'tool_name'ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    logger.info(f"ì‹¤í–‰í•  ë„êµ¬: {tool_name}, ì¸ìˆ˜: {tool_args}")
                    report_job_progress(job_id, log_message=f"Directly invoking tool: {tool_name}", percent_complete=30)
                    # Frontendê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ìˆë„ë¡ tool_invocationsì— ê¸°ë¡
                    report_tool_callback(job_id, tool_name, tool_args)


                    # ë„êµ¬ ì‹¤í–‰
                    tool_to_run = next((t for t in job_tools if t.name == tool_name), None)
                    if not tool_to_run:
                        raise ValueError(f"'{tool_name}'ì— í•´ë‹¹í•˜ëŠ” ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    result = tool_to_run.invoke(tool_args)

                    # ì‹¤í–‰ ê²°ê³¼ë¥¼ tool_invocationsì— ì—…ë°ì´íŠ¸
                    report_tool_callback(job_id, tool_name, tool_args, tool_output=ensure_jsonable(result))
                    logger.info(f"âœ… ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ. ê²°ê³¼ íƒ€ì…: {type(result)}")

                    # ë¶„ì„ ëŒ€ìƒ ë„êµ¬ì¸ ê²½ìš° LLMìœ¼ë¡œ ê²°ê³¼ ë¶„ì„
                    analysis_tools = ['calculate_loc_per_language', 'get_diff']
                    final_summary = None

                    if tool_name in analysis_tools:
                        logger.info(f"ë„êµ¬ ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ë¶„ì„ ì¤‘: {tool_name}")
                        report_job_progress(job_id, log_message=f"Analyzing tool output from {tool_name}...", percent_complete=70)

                        # ë„êµ¬ë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
                        analysis_prompts = {
                            'calculate_loc_per_language': """ë‹¤ìŒì€ ì €ì¥ì†Œì˜ ì–¸ì–´ë³„ ì½”ë“œ ë¼ì¸ ìˆ˜(LOC) ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.

ê²°ê³¼: {result}

ì´ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ë¶„ì„í•˜ê³  í•´ì„í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
- ì–´ë–¤ ì–¸ì–´ê°€ ê°€ì¥ ë§ì€ê°€?
- í”„ë¡œì íŠ¸ì˜ ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€?
- ê° ì–¸ì–´ì˜ ë¹„ìœ¨ì€ ì–´ëŠ ì •ë„ì¸ê°€?
""",
                            'get_diff': """ë‹¤ìŒì€ ì €ì¥ì†Œì˜ ë³€ê²½ ì‚¬í•­(Diff) ì¡°íšŒ ê²°ê³¼ì…ë‹ˆë‹¤.

ê²°ê³¼: {result}

ì´ Diffë¥¼ ìì—°ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
- ì–´ë–¤ íŒŒì¼ë“¤ì´ ë³€ê²½ë˜ì—ˆëŠ”ê°€?
- ì£¼ìš” ë³€ê²½ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€?
- ë³€ê²½ ê·œëª¨ëŠ” ì–´ëŠ ì •ë„ì¸ê°€?
"""
                        }

                        if tool_name in analysis_prompts:
                            analysis_prompt = analysis_prompts[tool_name].format(result=str(result))

                            try:
                                analysis_response = llm_for_analysis.invoke([
                                    HumanMessage(content=analysis_prompt)
                                ])
                                final_summary = analysis_response.content
                                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {tool_name}")
                            except Exception as e:
                                logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                                final_summary = str(result)
                        else:
                            final_summary = str(result)
                    else:
                        final_summary = str(result)

                    report_job_progress(job_id, log_message="Tool execution and analysis finished.", percent_complete=100)
                    report_job_status(job_id, 'complete', summary=final_summary, job_status='success')
                    
                    logger.info(f"ğŸ‰ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ Job {job_id} ì •ìƒ ì™„ë£Œ")

                except Exception as e:
                    logger.exception(f"âŒ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ Job {job_id} ì‹¤íŒ¨: {e}")
                    report_job_status(job_id, 'complete', summary=str(e), error_message=str(e), job_status='failed')
                
                finally:
                    send_heartbeat('idle')
                    continue # LLM í˜¸ì¶œ ë¡œì§ì„ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ë£¨í”„ë¡œ ì´ë™

            # --- ê¸°ì¡´ LLM ê¸°ë°˜ ì‘ì—… ì²˜ë¦¬ ---
            job_description = build_job_prompt(job_payload, job_type)
            
            # ì´ jobì„ ìœ„í•œ ìƒˆë¡œìš´ app ìƒì„±
            job_workflow = StateGraph(AgentState)
            job_workflow.add_node("agent", call_model)
            job_workflow.add_node("action", lambda state: call_tool_with_executor(state, job_tool_executor))
            job_workflow.add_node("analyze", lambda state: analyze_tool_results(state, llm_with_tools))
            job_workflow.set_entry_point("agent")
            job_workflow.add_conditional_edges(
                "agent",
                should_continue,
                {
                    "continue": "action",
                    "end": END,
                },
            )
            def should_analyze(state: AgentState):
                """ë„êµ¬ ì‹¤í–‰ í›„ ë¶„ì„ì´ í•„ìš”í•œì§€ íŒë‹¨"""
                # ë¶„ì„ ëŒ€ìƒ ë„êµ¬ ëª©ë¡
                analysis_tools = ['calculate_loc_per_language', 'get_diff']

                # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ToolMessageì¸ì§€ í™•ì¸
                if not state['messages']:
                    return "end"

                last_msg = state['messages'][-1]
                if not isinstance(last_msg, ToolMessage):
                    return "end"

                # AIMessageì—ì„œ tool_name ì°¾ê¸°
                tool_name = None
                for i in range(len(state['messages']) - 2, -1, -1):
                    msg = state['messages'][i]
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        # ë§ˆì§€ë§‰ ToolMessageì™€ ì¼ì¹˜í•˜ëŠ” tool_call ì°¾ê¸°
                        for tool_call in msg.tool_calls:
                            if tool_call.get('id') == last_msg.tool_call_id:
                                tool_name = tool_call.get('name')
                                logger.debug(f"ë¶„ì„ ê²°ì •: ë„êµ¬ëª…={tool_name}, ë¶„ì„ëŒ€ìƒ={tool_name in analysis_tools}")
                                return "analyze" if tool_name in analysis_tools else "end"

                logger.debug("ë„êµ¬ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ ìŠ¤í‚µ")
                return "end"

            job_workflow.add_conditional_edges(
                "action",
                should_analyze,
                {
                    "analyze": "analyze",
                    "end": END,
                }
            )
            job_workflow.add_edge("analyze", END)
            job_app = job_workflow.compile()

            send_heartbeat('assigned', current_job_id=job_id)
            report_job_status(job_id, 'start')
            job_metrics[job_id] = {"tool_calls": 0, "started_at": time.time()}
            logger.info(f"ğŸ”„ Job {job_id} ìˆ˜ë½ - Agent ì²˜ë¦¬ ì‹œì‘")
            report_job_progress(job_id, log_message="Job accepted by agent.", percent_complete=0)

            inputs = {
                'messages': [HumanMessage(content=job_description)],
                'job_id': str(job_id),
                'job_description': job_description,
                'job_payload': job_payload,
            }

            try:
                send_heartbeat('processing', current_job_id=job_id)
                logger.info(f"âš™ï¸ Job {job_id} ì‹¤í–‰ ì¤‘...")
                final_state = job_app.invoke(inputs)
                final_message = final_state['messages'][-1].content
                logger.debug(f"Job {job_id} ìµœì¢… ìƒíƒœ: {final_state}")


                logger.info(f"âœ… Job {job_id} ì‹¤í–‰ ì™„ë£Œ")
                logger.info(f"ğŸ“ ê²°ê³¼ ê¸¸ì´: {len(final_message)} ê¸€ì")

                report_job_progress(job_id, log_message="Job execution finished.", percent_complete=100)
                result_url = None
                metadata = job_payload.get('metadata')
                if isinstance(metadata, dict):
                    result_url = metadata.get('result_url')
                report_job_status(job_id, 'complete', summary=final_message, result_url=result_url)
                logger.info("=" * 80)
                logger.info(f"ğŸ‰ Job {job_id} ì •ìƒ ì™„ë£Œ")
                logger.info("=" * 80)

            except Exception as job_error:
                logger.exception(f"âŒ Job {job_id} ì‹¤íŒ¨: {job_error}")
                # ì‹¤íŒ¨ ìƒíƒœë¥¼ API ì„œë²„ì— ë³´ê³ 
                report_job_progress(job_id, log_message=f"Job failed: {job_error}")
                report_job_status(
                    job_id,
                    'complete',
                    summary=f"An unexpected error occurred: {job_error}",
                    error_message=str(job_error),
                    job_status='failed',
                )
                logger.info("=" * 80)
                logger.error(f"âŒ Job {job_id} ì˜¤ë¥˜ ì™„ë£Œ")
                logger.info("=" * 80)

            finally:
                metrics = job_metrics.pop(job_id, {})
                started_at = metrics.get('started_at') or time.time()
                metrics['duration_ms'] = int((time.time() - started_at) * 1000)
                metrics['tool_calls'] = metrics.get('tool_calls', 0)
                logger.info(f"ğŸ“Š Job {job_id} ë©”íŠ¸ë¦­ - ì†Œìš”ì‹œê°„: {metrics['duration_ms']}ms, Tool í˜¸ì¶œ: {metrics['tool_calls']}íšŒ")
                report_telemetry(job_id, metrics)
                send_heartbeat('idle')

        except requests.RequestException as exc:
            logger.error(f"Could not connect to API server: {exc}. Retrying in 30 seconds...")
            time.sleep(30)
        except Exception as exc:
            logger.error(f"An unexpected error occurred: {exc}", exc_info=True)
            time.sleep(10)


if __name__ == "__main__":
    print("Desktop Backend Agent is running. Press Ctrl+C to stop.")
    # run_agent()  # Execution entrypoint is managed elsewhere.
